{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run everything in this section to define functions and imports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports. \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the paths below for code to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "snare_path = '/home/rcorona/obj_part_lang/snare-master/amt/folds_adversarial'\n",
    "metadata_path = './data/metadata.csv'\n",
    "categories_path = './data/categories.synset.csv'\n",
    "lfn_feat_dir = './data/lfn_feats'\n",
    "clip_feat_dir = '/home/rcorona/dev/snare-master/data/shapenet-clipViT32-frames/'\n",
    "pixelnerf_feat_dir = '/home/rcorona/2022/lang_nerf/vlg/snare-master/data/pixelnerf_custom_feats'\n",
    "legoformer_feat_dir = '/home/rcorona/2022/lang_nerf/vlg/snare-master/data/legoformer_multiview_feats/'\n",
    "\n",
    "# TODO Set GPU number here. \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snare_objs(): \n",
    "    \"\"\"\n",
    "    Get all ShapeNetSem object IDs for objects used in SNARE. \n",
    "    \"\"\"\n",
    "    train = json.load(open(os.path.join(snare_path, 'train.json')))\n",
    "    val = json.load(open(os.path.join(snare_path, 'val.json')))\n",
    "    test = json.load(open(os.path.join(snare_path, 'test.json')))\n",
    "\n",
    "    train_objs = set()\n",
    "    val_objs = set()\n",
    "    test_objs = set()\n",
    "\n",
    "    # Comb through snare files to collect unique set of ShapeNet objects. \n",
    "    snare_objs = set()\n",
    "\n",
    "    for obj_set, split in [(train_objs, train), (val_objs, val), (test_objs, test)]:\n",
    "        for datapoint in split: \n",
    "            for obj in datapoint['objects']:\n",
    "                obj_set.add(obj)\n",
    "\n",
    "    all_objs = train_objs | val_objs | test_objs\n",
    "\n",
    "    return list(all_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_labels(objs):\n",
    "    \"\"\"\n",
    "    Generate category labels for TSNE plot. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Load all metadata for objects. \n",
    "    with open(metadata_path, 'r') as csvfile: \n",
    "        metadata = [row for row in csv.reader(csvfile)]\n",
    "        \n",
    "    # Get index of each object in metdata. \n",
    "    obj2synset = {m[0].replace('wss.', '').strip(): m[2].strip() for m in metadata}\n",
    "        \n",
    "    # Load synset word mappings. \n",
    "    with open(categories_path, 'r') as csvfile: \n",
    "        mappings = [row for row in csv.reader(csvfile)]\n",
    "        \n",
    "        # Mapping from synset to word. \n",
    "        s2w = {r[2].strip(): r[3].split(',')[0].strip() for r in mappings[1:]}    \n",
    "\n",
    "\n",
    "    # Get set of all synsets. \n",
    "    synset_codes = set([r[2].strip() for r in metadata[2:]])    \n",
    "    synsets = []\n",
    "\n",
    "    for s in synset_codes: \n",
    "        try: \n",
    "            synset = s2w[s]\n",
    "        except: \n",
    "            synset = 'None'\n",
    "            \n",
    "        synsets.append(synset)\n",
    "        \n",
    "\n",
    "    ## Get 10 most common object categories and filter out everything else.  \n",
    "    snare_synsets = []\n",
    "    counts = Counter()\n",
    "\n",
    "    # Count object categories. \n",
    "    for obj in objs:\n",
    "        synset = obj2synset[obj]\n",
    "\n",
    "        # Only count those with label. \n",
    "        if synset in s2w:\n",
    "            word = s2w[synset]\n",
    "            snare_synsets.append(word)\n",
    "            \n",
    "            # Update word count.\n",
    "            if not word == '': \n",
    "                counts[word] += 1\n",
    "        else: \n",
    "            snare_synsets.append(None)\n",
    "            \n",
    "    return snare_synsets, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lfn_features(objs):\n",
    "    \"\"\"\n",
    "    Given list of object IDs, load features for each object under LFN.  \n",
    "    \"\"\"\n",
    "    # Load all features and name order. \n",
    "    feats = []\n",
    "    \n",
    "    for obj in objs:\n",
    "        \n",
    "        # Load feature. \n",
    "        path = os.path.join(lfn_feat_dir, '{}.npy'.format(obj))\n",
    "        feat = feats.append(np.load(path))\n",
    "        \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clip_features(objs):\n",
    "    \"\"\"\n",
    "    Given list of object IDs, load features for each object under CLIP.  \n",
    "    \"\"\"\n",
    "    # Load all features and name order. \n",
    "    feats = []\n",
    "\n",
    "    for obj in objs:\n",
    "        \n",
    "        # Load image features for object. \n",
    "        path = os.path.join(clip_feat_dir, '{}.npy'.format(obj))\n",
    "        \n",
    "        # Load features for input views and take mean. \n",
    "        feat = np.mean(np.load(path)[6:], axis=0)\n",
    "        feats.append(feat)\n",
    "        \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_legoformer_features(objs):\n",
    "    \"\"\"\n",
    "    Given list of object IDs, load features for each object under LegoFormer.  \n",
    "    \"\"\"\n",
    "    # Load all features and name order. \n",
    "    feats = []\n",
    "\n",
    "    for obj in objs:\n",
    "        \n",
    "        # Load image features for object. \n",
    "        path = os.path.join(legoformer_feat_dir, '{}.npy'.format(obj))\n",
    "        \n",
    "        # Load and collapse dimension to consider as single feature. \n",
    "        feats.append(np.reshape(np.load(path), -1))\n",
    "        \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pixelnerf_features(objs):\n",
    "    \"\"\"\n",
    "    Given list of object IDs, load features for each object under PixelNeRF.  \n",
    "    \"\"\"\n",
    "    # Load all features and name order. \n",
    "    feats = []\n",
    "\n",
    "    print('Loading PixelNeRF features...')\n",
    "\n",
    "    for obj in tqdm(objs):\n",
    "        \n",
    "        # Load image features for object. \n",
    "        path = os.path.join(pixelnerf_feat_dir, '{}.npy'.format(obj))\n",
    "        \n",
    "        # Load features for input views and take mean. \n",
    "        feat = np.reshape(np.load(path), (8, 512, -1))\n",
    "        feat = np.mean(np.mean(feat, axis=0), axis=-1)\n",
    "        feats.append(feat)\n",
    "        \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_top_k(counts, feats, objs, snare_synsets):\n",
    "    # Get 10 most common categories. \n",
    "    top10 = {t[0]: t[1] for t in counts.most_common(10)}\n",
    "\n",
    "    # Only keep objects in top-10 categories. \n",
    "    final_feats = []\n",
    "    final_labels = []\n",
    "\n",
    "    assert len(objs) == len(feats) and len(objs) == len(snare_synsets)\n",
    "\n",
    "    for i in range(len(objs)):\n",
    "        synset = snare_synsets[i]\n",
    "        \n",
    "        if synset in top10: \n",
    "            final_feats.append(feats[i])\n",
    "            final_labels.append(synset)\n",
    "            \n",
    "    # Create numpy array of features. \n",
    "    final_feats = np.stack(final_feats)\n",
    "    print('Final feature shape: {}'.format(final_feats.shape))\n",
    "    \n",
    "    return final_feats, final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of all ShapeNetSem objects used in SNARE. \n",
    "snare_objs = get_snare_objs()\n",
    "\n",
    "# Generate category labels for TSNE plot. \n",
    "synsets, counts = get_plot_labels(snare_objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Probe Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for linear probes. \n",
    "class ProbeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, feats, labels, idx_dict):\n",
    "        \n",
    "        # List of objects in this dataset split. \n",
    "        self.feats = feats\n",
    "        \n",
    "        # Synset labels for objects. \n",
    "        self.labels = labels\n",
    "        \n",
    "        # Holds ID mappings for labels. \n",
    "        self.idx_dict = idx_dict\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.feats)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        \n",
    "        # Load object features and label. \n",
    "        feat = torch.Tensor(self.feats[idx]).float()\n",
    "        label = self.idx_dict[self.labels[idx]]\n",
    "        \n",
    "        return feat, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbe(nn.Module):\n",
    "    \n",
    "    def __init__(self, feat_dim, n_categories, mlp=False):\n",
    "        super().__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "        self.n_categories = n_categories\n",
    "        \n",
    "        # Simple linear probe. \n",
    "        if not mlp: \n",
    "            self.probe = nn.Linear(feat_dim, n_categories)\n",
    "        else: \n",
    "            self.probe = nn.Sequential(\n",
    "                nn.Linear(feat_dim, feat_dim // 2), \n",
    "                nn.ReLU(), \n",
    "                nn.Linear(feat_dim // 2, n_categories)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.probe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop. \n",
    "def train_loop(model, train_dataloader):\n",
    "    \n",
    "    # Initialize optimizer. \n",
    "    optim = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    # Place model on train mode. \n",
    "    model.train()\n",
    "    \n",
    "    # Do one epoch of updates. \n",
    "    for feats, labels in train_dataloader:\n",
    "        \n",
    "        # Put on GPU. \n",
    "        feats = feats.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        # Zero out gradients on optimizer. \n",
    "        optim.zero_grad()\n",
    "        \n",
    "        # Forward pass. \n",
    "        logits = model(feats)\n",
    "        \n",
    "        # Compute CE loss and take update.  \n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        loss.backward()\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model accuracy on a dataset split. \n",
    "def eval_model(model, loader):\n",
    "    \n",
    "    # Keep track of accuracy across all datapoints. \n",
    "    all_correct = []\n",
    "    \n",
    "    # Place model on eval mode. \n",
    "    model.eval()\n",
    "    \n",
    "    # Go over entire dataset. \n",
    "    with torch.no_grad():\n",
    "        for feats, labels in loader:\n",
    "            \n",
    "            # Put on GPU. \n",
    "            feats = feats.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "            # Forward pass and prediction. \n",
    "            logits = model(feats)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            \n",
    "            # Compute accuracy across batch and add to list over dataset. \n",
    "            correct = torch.eq(preds, labels).long()\n",
    "            all_correct.append(correct.cpu().numpy())\n",
    "            \n",
    "    # Compute dataset split accuracy. \n",
    "    all_correct = np.concatenate(all_correct)\n",
    "    acc = np.mean(all_correct)\n",
    "    \n",
    "    return acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training pipeline. \n",
    "def run_probe(model, train_loader, val_loader, test_loader, n_epochs):\n",
    "    \n",
    "    # Keep track of best model checkpoint for test set. \n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        # Do a training iteration with model. \n",
    "        train_loop(model, train_loader)\n",
    "        \n",
    "        # Evaluate model on validation set. \n",
    "        val_acc = eval_model(model, val_loader)\n",
    "        \n",
    "        # Keep best performing model checkpoint. \n",
    "        if val_acc > best_acc: \n",
    "            torch.save(model.state_dict(), 'probe.pth')\n",
    "            best_acc = val_acc\n",
    "        \n",
    "        # Print best accuracy. \n",
    "        print('Best Acc: {}'.format(best_acc))\n",
    "            \n",
    "    # Evaluate best checkpoint on test set.\n",
    "    model.load_state_dict(torch.load('probe.pth')) \n",
    "    test_acc = eval_model(model, test_loader)\n",
    "    print('Probe test performance: {}'.format(test_acc))\n",
    "    \n",
    "    # Get rid of probe temp path. \n",
    "    os.remove('probe.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_probe(snare_objs, counts, synsets, feat_load_func, feat_dim, mlp=False):\n",
    "\n",
    "    # General hyperparameters. \n",
    "    batch_size = 64\n",
    "    n_epochs = 100\n",
    "    n_categories = 10\n",
    "\n",
    "    # Load features and labels.\n",
    "    feats = feat_load_func(snare_objs)\n",
    "\n",
    "    # Filter features by top 10 most occurring categories. \n",
    "    final_feats, final_labels = filter_by_top_k(counts, feats, snare_objs, synsets)\n",
    "\n",
    "    # Compute dataset split lengths. \n",
    "    train_len = int(float(len(final_feats)) * 0.8)\n",
    "    val_len = int(float(len(final_feats)) * 0.1)\n",
    "\n",
    "    # Compute ID dictionary for labels. \n",
    "    label_list = list(set(final_labels))\n",
    "    idx_dict = {label_list[i]: i for i in range(len(label_list))} \n",
    "\n",
    "    # Split into datasets. \n",
    "    train_dataset = ProbeDataset(final_feats[:train_len], final_labels[:train_len], idx_dict)\n",
    "    val_dataset = ProbeDataset(final_feats[train_len:train_len+val_len], final_labels[train_len:train_len+val_len], idx_dict)\n",
    "    test_dataset = ProbeDataset(final_feats[train_len+val_len:], final_labels[train_len+val_len:], idx_dict)\n",
    "\n",
    "    # Form dataloaders. \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Instantiate model. \n",
    "    model = LinearProbe(feat_dim, 10, mlp=mlp)\n",
    "    model.cuda()\n",
    "    \n",
    "    # Run linear probe. \n",
    "    run_probe(model, train_loader, val_loader, test_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Linear Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CLIP Linear Probe...\n",
      "Final feature shape: (2273, 512)\n",
      "Best Acc: 0.6784140969162996\n",
      "Best Acc: 0.8061674008810573\n",
      "Best Acc: 0.8281938325991189\n",
      "Best Acc: 0.8546255506607929\n",
      "Best Acc: 0.8590308370044053\n",
      "Best Acc: 0.8722466960352423\n",
      "Best Acc: 0.8722466960352423\n",
      "Best Acc: 0.8766519823788547\n",
      "Best Acc: 0.8766519823788547\n",
      "Best Acc: 0.8766519823788547\n",
      "Best Acc: 0.8766519823788547\n",
      "Best Acc: 0.8766519823788547\n",
      "Best Acc: 0.8766519823788547\n",
      "Best Acc: 0.8766519823788547\n",
      "Best Acc: 0.8766519823788547\n",
      "Best Acc: 0.8766519823788547\n",
      "Best Acc: 0.8766519823788547\n",
      "Best Acc: 0.8766519823788547\n",
      "Best Acc: 0.8766519823788547\n",
      "Best Acc: 0.8766519823788547\n",
      "Best Acc: 0.8766519823788547\n",
      "Best Acc: 0.8766519823788547\n",
      "Best Acc: 0.8766519823788547\n",
      "Best Acc: 0.8766519823788547\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Best Acc: 0.8810572687224669\n",
      "Probe test performance: 0.9035087719298246\n"
     ]
    }
   ],
   "source": [
    "print('Training CLIP Linear Probe...')\n",
    "linear_probe(snare_objs, counts, synsets, load_clip_features, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLG (LegoFormer) Linear Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LegoFormer Linear Probe...\n",
      "Final feature shape: (2273, 1152)\n",
      "Best Acc: 0.4977973568281938\n",
      "Best Acc: 0.5638766519823789\n",
      "Best Acc: 0.5638766519823789\n",
      "Best Acc: 0.6123348017621145\n",
      "Best Acc: 0.6387665198237885\n",
      "Best Acc: 0.6387665198237885\n",
      "Best Acc: 0.6431718061674009\n",
      "Best Acc: 0.6696035242290749\n",
      "Best Acc: 0.6696035242290749\n",
      "Best Acc: 0.6784140969162996\n",
      "Best Acc: 0.6784140969162996\n",
      "Best Acc: 0.7004405286343612\n",
      "Best Acc: 0.7004405286343612\n",
      "Best Acc: 0.7004405286343612\n",
      "Best Acc: 0.7004405286343612\n",
      "Best Acc: 0.7004405286343612\n",
      "Best Acc: 0.7004405286343612\n",
      "Best Acc: 0.7004405286343612\n",
      "Best Acc: 0.7004405286343612\n",
      "Best Acc: 0.7004405286343612\n",
      "Best Acc: 0.7048458149779736\n",
      "Best Acc: 0.7048458149779736\n",
      "Best Acc: 0.7048458149779736\n",
      "Best Acc: 0.7048458149779736\n",
      "Best Acc: 0.7048458149779736\n",
      "Best Acc: 0.7092511013215859\n",
      "Best Acc: 0.7092511013215859\n",
      "Best Acc: 0.7092511013215859\n",
      "Best Acc: 0.7180616740088106\n",
      "Best Acc: 0.7180616740088106\n",
      "Best Acc: 0.7180616740088106\n",
      "Best Acc: 0.7180616740088106\n",
      "Best Acc: 0.7180616740088106\n",
      "Best Acc: 0.7180616740088106\n",
      "Best Acc: 0.7180616740088106\n",
      "Best Acc: 0.7224669603524229\n",
      "Best Acc: 0.7224669603524229\n",
      "Best Acc: 0.7224669603524229\n",
      "Best Acc: 0.7224669603524229\n",
      "Best Acc: 0.7224669603524229\n",
      "Best Acc: 0.7224669603524229\n",
      "Best Acc: 0.7224669603524229\n",
      "Best Acc: 0.7224669603524229\n",
      "Best Acc: 0.7224669603524229\n",
      "Best Acc: 0.7224669603524229\n",
      "Best Acc: 0.7268722466960352\n",
      "Best Acc: 0.7268722466960352\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Best Acc: 0.7312775330396476\n",
      "Probe test performance: 0.7763157894736842\n"
     ]
    }
   ],
   "source": [
    "print('Training LegoFormer Linear Probe...')\n",
    "linear_probe(snare_objs, counts, synsets, load_legoformer_features, 96 * 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PixelNeRF Linear Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PixelNeRF Linear Probe...\n",
      "Loading PixelNeRF features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7881/7881 [03:48<00:00, 34.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature shape: (2273, 512)\n",
      "Best Acc: 0.29515418502202645\n",
      "Best Acc: 0.3436123348017621\n",
      "Best Acc: 0.4669603524229075\n",
      "Best Acc: 0.47577092511013214\n",
      "Best Acc: 0.47577092511013214\n",
      "Best Acc: 0.47577092511013214\n",
      "Best Acc: 0.5594713656387665\n",
      "Best Acc: 0.5594713656387665\n",
      "Best Acc: 0.5594713656387665\n",
      "Best Acc: 0.5594713656387665\n",
      "Best Acc: 0.5814977973568282\n",
      "Best Acc: 0.6299559471365639\n",
      "Best Acc: 0.6916299559471366\n",
      "Best Acc: 0.6916299559471366\n",
      "Best Acc: 0.6916299559471366\n",
      "Best Acc: 0.7004405286343612\n",
      "Best Acc: 0.7092511013215859\n",
      "Best Acc: 0.7092511013215859\n",
      "Best Acc: 0.7400881057268722\n",
      "Best Acc: 0.7709251101321586\n",
      "Best Acc: 0.7709251101321586\n",
      "Best Acc: 0.7709251101321586\n",
      "Best Acc: 0.7797356828193832\n",
      "Best Acc: 0.7797356828193832\n",
      "Best Acc: 0.7797356828193832\n",
      "Best Acc: 0.7797356828193832\n",
      "Best Acc: 0.7797356828193832\n",
      "Best Acc: 0.7797356828193832\n",
      "Best Acc: 0.7797356828193832\n",
      "Best Acc: 0.7841409691629956\n",
      "Best Acc: 0.7841409691629956\n",
      "Best Acc: 0.7841409691629956\n",
      "Best Acc: 0.7841409691629956\n",
      "Best Acc: 0.7841409691629956\n",
      "Best Acc: 0.7929515418502202\n",
      "Best Acc: 0.7929515418502202\n",
      "Best Acc: 0.7929515418502202\n",
      "Best Acc: 0.7929515418502202\n",
      "Best Acc: 0.7929515418502202\n",
      "Best Acc: 0.7929515418502202\n",
      "Best Acc: 0.7973568281938326\n",
      "Best Acc: 0.7973568281938326\n",
      "Best Acc: 0.7973568281938326\n",
      "Best Acc: 0.7973568281938326\n",
      "Best Acc: 0.7973568281938326\n",
      "Best Acc: 0.8061674008810573\n",
      "Best Acc: 0.8061674008810573\n",
      "Best Acc: 0.8061674008810573\n",
      "Best Acc: 0.8061674008810573\n",
      "Best Acc: 0.8061674008810573\n",
      "Best Acc: 0.8061674008810573\n",
      "Best Acc: 0.8061674008810573\n",
      "Best Acc: 0.8061674008810573\n",
      "Best Acc: 0.8061674008810573\n",
      "Best Acc: 0.8061674008810573\n",
      "Best Acc: 0.8105726872246696\n",
      "Best Acc: 0.8105726872246696\n",
      "Best Acc: 0.8105726872246696\n",
      "Best Acc: 0.8105726872246696\n",
      "Best Acc: 0.8105726872246696\n",
      "Best Acc: 0.8105726872246696\n",
      "Best Acc: 0.8105726872246696\n",
      "Best Acc: 0.8105726872246696\n",
      "Best Acc: 0.8105726872246696\n",
      "Best Acc: 0.8105726872246696\n",
      "Best Acc: 0.8105726872246696\n",
      "Best Acc: 0.8105726872246696\n",
      "Best Acc: 0.8105726872246696\n",
      "Best Acc: 0.8105726872246696\n",
      "Best Acc: 0.8105726872246696\n",
      "Best Acc: 0.8105726872246696\n",
      "Best Acc: 0.8105726872246696\n",
      "Best Acc: 0.8193832599118943\n",
      "Best Acc: 0.8193832599118943\n",
      "Best Acc: 0.8193832599118943\n",
      "Best Acc: 0.8193832599118943\n",
      "Best Acc: 0.8193832599118943\n",
      "Best Acc: 0.8193832599118943\n",
      "Best Acc: 0.8193832599118943\n",
      "Best Acc: 0.8193832599118943\n",
      "Best Acc: 0.8193832599118943\n",
      "Best Acc: 0.8281938325991189\n",
      "Best Acc: 0.8281938325991189\n",
      "Best Acc: 0.8370044052863436\n",
      "Best Acc: 0.8370044052863436\n",
      "Best Acc: 0.8370044052863436\n",
      "Best Acc: 0.8370044052863436\n",
      "Best Acc: 0.8370044052863436\n",
      "Best Acc: 0.8370044052863436\n",
      "Best Acc: 0.8370044052863436\n",
      "Best Acc: 0.8370044052863436\n",
      "Best Acc: 0.8370044052863436\n",
      "Best Acc: 0.8370044052863436\n",
      "Best Acc: 0.8370044052863436\n",
      "Best Acc: 0.8370044052863436\n",
      "Best Acc: 0.8370044052863436\n",
      "Best Acc: 0.8370044052863436\n",
      "Best Acc: 0.8370044052863436\n",
      "Best Acc: 0.8370044052863436\n",
      "Best Acc: 0.8370044052863436\n",
      "Probe test performance: 0.8070175438596491\n"
     ]
    }
   ],
   "source": [
    "print('Training PixelNeRF Linear Probe...')\n",
    "linear_probe(snare_objs, counts, synsets, load_pixelnerf_features, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LFN Linear Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LFN Linear Probe...\n",
      "Final feature shape: (2273, 256)\n",
      "Best Acc: 0.15859030837004406\n",
      "Best Acc: 0.15859030837004406\n",
      "Best Acc: 0.18502202643171806\n",
      "Best Acc: 0.18502202643171806\n",
      "Best Acc: 0.18502202643171806\n",
      "Best Acc: 0.18502202643171806\n",
      "Best Acc: 0.18502202643171806\n",
      "Best Acc: 0.18502202643171806\n",
      "Best Acc: 0.18502202643171806\n",
      "Best Acc: 0.18502202643171806\n",
      "Best Acc: 0.2026431718061674\n",
      "Best Acc: 0.2026431718061674\n",
      "Best Acc: 0.21145374449339208\n",
      "Best Acc: 0.21145374449339208\n",
      "Best Acc: 0.21145374449339208\n",
      "Best Acc: 0.21145374449339208\n",
      "Best Acc: 0.21145374449339208\n",
      "Best Acc: 0.21145374449339208\n",
      "Best Acc: 0.21585903083700442\n",
      "Best Acc: 0.21585903083700442\n",
      "Best Acc: 0.22026431718061673\n",
      "Best Acc: 0.22026431718061673\n",
      "Best Acc: 0.22026431718061673\n",
      "Best Acc: 0.22026431718061673\n",
      "Best Acc: 0.22026431718061673\n",
      "Best Acc: 0.22026431718061673\n",
      "Best Acc: 0.22026431718061673\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Best Acc: 0.22466960352422907\n",
      "Probe test performance: 0.16228070175438597\n"
     ]
    }
   ],
   "source": [
    "print('Training LFN Linear Probe...')\n",
    "linear_probe(snare_objs, counts, synsets, load_lfn_features, 256, mlp=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('snare_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78dba385ee9abf9152c787729aa5a448da84ca902d6733d7ee16359c358ffbc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
